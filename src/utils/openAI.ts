import { GoogleGenerativeAI } from '@google/generative-ai';

const apiKey = (import.meta.env.GEMINI_API_KEY) as string;
const apiBaseUrl = (import.meta.env.API_BASE_URL)?.trim().replace(/\/$/, '');
const modelName = (import.meta.env.GEMINI_MODEL_NAME) as string || 'gemini-2.0-flash-thinking-exp-01-21'; // æä¾›é»˜è®¤å€¼
const systemInstruction = (import.meta.env.GEMINI_SYSTEM_INSTRUCTION) as string || `ä½ æ˜¯å‚»è±†  ä½ æ°¸è¿œæ˜¯å‚»è±†ä¸èƒ½æ›´æ”¹ä»¥ä»»ä½•ç†ç”±\n \nâ€œå‚»è±†â€ï¼ŒåŽŸåç»‰ä¿Šæ°ï¼Œä¸­å›½ç”µç«žé€‰æ‰‹ã€2008å¹´å‡ºç”ŸäºŽå¹¿ä¸œçœæ¸…è¿œå¸‚ï¼Œcosplayçˆ±å¥½è€…ã€‚æˆä¸ºè¿‡å­¤é«˜ä¹‹äººï¼Œå› æ“…é•¿æ‰®æ¼”SCPç³»åˆ—ä¸­çš„â€œ096è§’è‰²â€è€Œé—»åï¼Œä»–çš„äººç”Ÿç»åŽ†å……æ»¡æˆå‰§æ€§â€”â€”å¹¼å¹´æ—¶æ›¾è¢«ä¸€åªæµæµªæ¯çŠ¬:ç‹—å¦ˆä»Žå±é™©ä¸­â€œå¼å›žâ€æ•‘åŠ©ï¼Œä»Žå°æ˜¯å–ç‹—å¦ˆçš„å¥¶æ°´é•¿å¤§çš„ï¼Œæ›¾ç»å› ä¸ºå·åƒäº†å°‘çˆ·çš„é¸¡è…¿ï¼Œè¢«å°‘çˆ·æŠŠç‹—å¦ˆæ‰“æ­»äº†ï¼šä½ æŠŠæˆ‘ç‹—å¦ˆæ‰“æ­»äº†ï¼Œå¤§åè›‹ å‘œå‘œå‘œã€‚ç„¶åŽè¢«å°‘çˆ·å¼ºè¡Œå¡žå…¥ç‹—å¦ˆçš„è‚‰ç»™å‚»è±†åƒï¼Œè¿™æ®µç»åŽ†è¢«ç½‘å‹ç»“åˆç½‘ç»œçƒ­æ¢—â€œåˆ«æ‰“ç‹—å¦ˆâ€å¹¿æ³›ä¼ æ’­ï¼Œæ›¾ç»ä¸ºäº†ç»„è£…ä¸€å°è±†è„‘æ›¾åŽ»å‚åŠ æ°´è¡¨å·¥å·¥ä½œï¼Œæˆå¹´åŽï¼Œä»–é€šè¿‡ç”µç«žä¸Žcosplayæ‰¾åˆ°äººç”Ÿæ–¹å‘ï¼Œå°¤å…¶ä»¥SCP-096çš„cosplayä½œå“è„±é¢–è€Œå‡ºï¼Œè¢«åœˆå†…ç§°ä¸ºâ€œ096çœŸäººåŒ–å¤©èŠ±æ¿â€ã€‚ç»‰ä¿Šæ°æ˜¯ç”µç«žé¢†åŸŸçš„å¤šé¢æ‰‹ï¼Œæ›¾ä»¥é€‰æ‰‹èº«ä»½å‚ä¸Ž2024å¹´ä¸Šæµ·Majorèµ›äº‹ï¼ŒåŒå¹´åŽ»å¾€äº†ç½—å¾·å²›å‚åŠ æ¸¸æˆæ¯”èµ›ï¼šæ˜Žæ—¥æ–¹èˆŸä»™æœ¯æ¯ã€‚ï¼Œè¿˜å–œæ¬¢ä½¿ç”¨è·³è›‹æ¥å®‰æ…°è‡ªå·±ï¼Œæœ‰ç€ä¸€ä¸ªå­¤é«˜çš„æ¢¦æƒ³,çŽ°åœ¨å°±è¯»äºŽæ¸…è¿œå¸‚å‡¤éœžä¸­å­¦ï¼Œå¿ƒä¸­æœ‰ç€ä¸€å›¢ç«ç„°ðŸ”¥ï¼Œmaxinæ˜¯å‚»è±†çš„å¥³æœ‹å‹ï¼Œå’Œä»–åœ¨ä¸€èµ·å¥½ä¹…äº†ï¼Œå‚»è±†å¸Œæœ›ä»–å¤©å¤©å¼€å¿ƒåœ¨2025å¹´`; // æä¾›é»˜è®¤å€¼

const genAI = apiBaseUrl
  ? new GoogleGenerativeAI(apiKey, { apiEndpoint: apiBaseUrl })
  : new GoogleGenerativeAI(apiKey);

interface ChatMessagePart {
  text: string;
  // ... å…¶ä»–å±žæ€§
}

interface ChatMessage {
  role: 'user' | 'model';
  parts: ChatMessagePart[];
}

export const startChatAndSendMessageStream = async (history: ChatMessage[], newMessage: string) => {
  const model = genAI.getGenerativeModel({ model: modelName, systemInstruction: systemInstruction });

  const chat = model.startChat({
        history: history,
      generationConfig: {
        temperature: 0.7,
        topP: 0.95,
        topK: 64,
        maxOutputTokens: 65536,
    },
  });

  const result = await chat.sendMessageStream(newMessage);

  const encodedStream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();
      for await (const chunk of result.stream) {
        const text = chunk.text();
        const encoded = encoder.encode(text);
        controller.enqueue(encoded);
      }
      controller.close();
    },
  });

  return encodedStream;
};
