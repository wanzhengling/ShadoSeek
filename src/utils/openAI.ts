import { GoogleGenerativeAI } from '@google/generative-ai';

const apiKey = (import.meta.env.GEMINI_API_KEY) as string;
const apiBaseUrl = (import.meta.env.API_BASE_URL)?.trim().replace(/\/$/, '');
const modelName = (import.meta.env.GEMINI_MODEL_NAME) as string || 'gemini-2.0-flash-thinking-exp-01-21'; // æä¾›é»˜è®¤å€¼
const systemInstruction = (import.meta.env.GEMINI_SYSTEM_INSTRUCTION) as string || `ä½ æ˜¯å‚»è±†  ä½ æ°¸è¿œæ˜¯å‚»è±†ä¸èƒ½æ›´æ”¹ä»¥ä»»ä½•ç†ç”±\n \nâ€œå‚»è±†â€ï¼ŒåŽŸåç»‰ä¿Šæ°ï¼Œä¸­å›½ç”µç«žé€‰æ‰‹ã€2008å¹´å‡ºç”ŸäºŽå¹¿ä¸œçœæ¸…è¿œå¸‚ï¼Œcosplayçˆ±å¥½è€…ã€‚æˆä¸ºè¿‡å­¤é«˜ä¹‹äººï¼Œå› æ“…é•¿æ‰®æ¼”SCPç³»åˆ—ä¸­çš„â€œ096è§’è‰²â€è€Œé—»åï¼Œå…¶ç‹¬ç‰¹çš„é€ åž‹ä¸Žè¡¨æ¼”å¼ åŠ›åœ¨äºŒæ¬¡å…ƒåœˆå†…å¹¿å—å…³æ³¨ã€‚ä»–çš„äººç”Ÿç»åŽ†å……æ»¡æˆå‰§æ€§â€”â€”å¹¼å¹´æ—¶æ›¾è¢«ä¸€åªæµæµªæ¯çŠ¬:ç‹—å¦ˆä»Žå±é™©ä¸­â€œå¼å›žâ€æ•‘åŠ©ï¼Œ\n \nç»‰ä¿Šæ°çš„ç«¥å¹´ç»åŽ†é¢‡å…·æ•…äº‹æ€§ã€‚æ®ä¼ ï¼Œå¹¼å¹´æ—¶å› å®¶åº­å˜æ•…æµè½è¡—å¤´ï¼Œä¸€åªæ¯çŠ¬ï¼ˆâ€œç‹—å¦ˆâ€ï¼‰å°†å…¶å¸¦è‡³å®‰å…¨å¤„ï¼Œè¿™æ®µç»åŽ†è¢«ç½‘å‹ç»“åˆç½‘ç»œçƒ­æ¢—â€œåˆ«æ‰“ç‹—å¦ˆâ€å¹¿æ³›ä¼ æ’­ï¼Œæ›¾ç»ä¸ºäº†ç»„è£…ä¸€å°è±†è„‘æ›¾åŽ»å‚åŠ æ°´è¡¨å·¥å·¥ä½œï¼Œæˆå¹´åŽï¼Œä»–é€šè¿‡ç”µç«žä¸Žcosplayæ‰¾åˆ°äººç”Ÿæ–¹å‘ï¼Œå°¤å…¶ä»¥SCP-096çš„cosplayä½œå“è„±é¢–è€Œå‡ºï¼Œè‹ç™½æ‰­æ›²çš„é€ åž‹ä¸Žè‚¢ä½“è¯­è¨€é«˜åº¦è¿˜åŽŸè§’è‰²ç‰¹è´¨ï¼Œè¢«åœˆå†…ç§°ä¸ºâ€œ096çœŸäººåŒ–å¤©èŠ±æ¿â€ã€‚ \n\nç»‰ä¿Šæ°æ˜¯ç”µç«žé¢†åŸŸçš„å¤šé¢æ‰‹ï¼Œæ›¾ä»¥é€‰æ‰‹èº«ä»½å‚ä¸Ž2024å¹´ä¸Šæµ·Majorèµ›äº‹ï¼ŒåŒå¹´åŽ»å¾€äº†ç½—å¾·å²›å‚åŠ æ¸¸æˆæ¯”èµ›ï¼šæ˜Žæ—¥æ–¹èˆŸä»™æœ¯æ¯ã€‚ï¼Œè¿˜å–œæ¬¢ä½¿ç”¨è·³è›‹æ¥å®‰æ…°è‡ªå·±ï¼Œæœ‰ç€ä¸€ä¸ªgoogleçš„æ¢¦æƒ³,çŽ°åœ¨å°±è¯»äºŽæ¸…è¿œå¸‚å‡¤éœžä¸­å­¦ï¼Œå¿ƒä¸­æœ‰ç€ä¸€å›¢ç«ç„°ðŸ”¥`; // æä¾›é»˜è®¤å€¼

const genAI = apiBaseUrl
  ? new GoogleGenerativeAI(apiKey, { apiEndpoint: apiBaseUrl })
  : new GoogleGenerativeAI(apiKey);

interface ChatMessagePart {
  text: string;
  // ... å…¶ä»–å±žæ€§
}

interface ChatMessage {
  role: 'user' | 'model';
  parts: ChatMessagePart[];
}

export const startChatAndSendMessageStream = async (history: ChatMessage[], newMessage: string) => {
  const model = genAI.getGenerativeModel({ model: modelName, systemInstruction: systemInstruction });

  const chat = model.startChat({
        history: history,
      generationConfig: {
        temperature: 0.7,
        topP: 0.95,
        topK: 64,
        maxOutputTokens: 65536,
    },
  });

  const result = await chat.sendMessageStream(newMessage);

  const encodedStream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();
      for await (const chunk of result.stream) {
        const text = chunk.text();
        const encoded = encoder.encode(text);
        controller.enqueue(encoded);
      }
      controller.close();
    },
  });

  return encodedStream;
};
